# Web Scraper fÃ¼r Ratsinformationssysteme (RIS)

Ein robuster, konfigurierbarer Scraper fÃ¼r deutsche Ratsinformationssysteme.

## ğŸ¯ Features

- âœ… **Multi-System Support**: ALLRIS, SessionNet, eSitzungsdienst, generisch
- âœ… **Auto-Detection**: Automatische Erkennung des RIS-Typs
- âœ… **Robustheit**: Retry-Logic, Rate Limiting, Error Handling
- âœ… **Fortschritt-Tracking**: Speichert Fortschritt, kann unterbrochen werden
- âœ… **Strukturierte Ausgabe**: HTML + JSON fÃ¼r jede Sitzung
- âœ… **Analyse-Tools**: Verstehen Sie die HTML-Struktur vor dem Scraping

## ğŸ“¦ Installation

```bash
# In Ihr Projekt-Verzeichnis wechseln
cd /Users/benedikt.pilgram/Code/Geomodelierung/stadtrat-etl-pipeline

# Dependencies installieren
pip install requests beautifulsoup4 lxml tqdm pyyaml

# Oder mit requirements.txt (falls vorhanden)
pip install -r requirements_scraper.txt
```

## ğŸš€ Quick Start

### 1. HTML-Struktur analysieren (WICHTIG!)

Bevor Sie scrapen, analysieren Sie die Struktur:

```bash
python analyze_ris_structure.py https://ratsinfo.IhreStadt.de
```

Dies zeigt Ihnen:
- RIS-Typ (ALLRIS, SessionNet, etc.)
- HTML-Struktur
- Wichtige CSS-Selektoren
- Link-Patterns
- Datumsformate

### 2. Ersten Test-Scrape durchfÃ¼hren

```bash
# Nur 5 Seiten zum Testen
python ris_scraper.py https://ratsinfo.IhreStadt.de -n 5 -v

# Mit Angabe des RIS-Typs (wenn bekannt)
python ris_scraper.py https://ratsinfo.IhreStadt.de --ris-type allris -n 10
```

### 3. VollstÃ¤ndiges Scraping

```bash
# Alle Sitzungen scrapen (max. 100 Seiten)
python ris_scraper.py https://ratsinfo.IhreStadt.de -n 100 -o ./data/meine_stadt

# Mit Zeitfilter
python ris_scraper.py https://ratsinfo.IhreStadt.de \
    --date-from 2024-01-01 \
    --date-to 2024-12-31 \
    -n 200
```

## ğŸ“– Detaillierte Nutzung

### Basis-Scraping

```bash
python ris_scraper.py <URL> [OPTIONEN]
```

**Argumente:**

| Argument | Beschreibung | Standard | Beispiel |
|----------|--------------|----------|----------|
| `url` | Basis-URL des RIS | *erforderlich* | `https://ratsinfo.stadt.de` |
| `-o, --output` | Output-Verzeichnis | `./scraped_data` | `-o ./meine_daten` |
| `-n, --max-pages` | Max. Anzahl Seiten | `50` | `-n 100` |
| `-r, --rate-limit` | Pause zwischen Requests (s) | `1.0` | `-r 2.0` |
| `--ris-type` | RIS-Typ (falls bekannt) | *auto-detect* | `--ris-type allris` |
| `--date-from` | Sitzungen ab Datum | - | `--date-from 2024-01-01` |
| `--date-to` | Sitzungen bis Datum | - | `--date-to 2024-12-31` |
| `-v, --verbose` | Debug-Modus | - | `-v` |

### Beispiele

**MÃ¼nchen ALLRIS scrapen:**
```bash
python ris_scraper.py https://risi.muenchen.de/risi \
    --ris-type allris \
    -n 200 \
    -o ./data/muenchen
```

**KÃ¶ln SessionNet mit Zeitfilter:**
```bash
python ris_scraper.py https://ratsinformation.stadt-koeln.de \
    --ris-type sessionnet \
    --date-from 2024-01-01 \
    --date-to 2024-12-31 \
    -o ./data/koeln
```

**Unbekanntes System (Generic Scraper):**
```bash
python ris_scraper.py https://www.kleinstadt.de/ratsinformation \
    --ris-type generic \
    -n 50 \
    -r 2.0 \
    -v
```

## ğŸ”§ Anpassung an Ihr RIS

Der Scraper muss an die spezifische HTML-Struktur angepasst werden.

### Schritt 1: Struktur analysieren

```bash
python analyze_ris_structure.py https://ratsinfo.IhreStadt.de > struktur_analyse.txt
```

Lesen Sie die Ausgabe sorgfÃ¤ltig und notieren Sie:
- RIS-Typ
- Link-Patterns fÃ¼r Sitzungen
- CSS-Selektoren fÃ¼r:
  - Sitzungs-Titel
  - Datum
  - Uhrzeit
  - Ort
  - Gremium
  - Tagesordnungspunkte

### Schritt 2: Scraper anpassen

Ã–ffnen Sie `ris_scraper.py` und passen Sie die relevante Scraper-Klasse an:

#### FÃ¼r ALLRIS-Systeme:

```python
class ALLRISScraper(BaseRISScraper):
    
    def scrape_meeting(self, url: str) -> Optional[ScrapedMeeting]:
        # ... (siehe Code)
        
        # ANPASSEN: Ihre Selektoren hier
        
        # Beispiel: Titel aus spezifischem Element
        title = soup.find('h1', class_='meeting-title')
        if title:
            title = title.get_text(strip=True)
        
        # Beispiel: Datum aus Tabellenzelle
        for td in soup.find_all('td', class_='datum'):
            date_text = td.get_text(strip=True)
            # ... Datum parsen
        
        # ... etc
```

### Schritt 3: Testen & Iterieren

```bash
# Test mit 1 Seite, verbose Output
python ris_scraper.py <URL> -n 1 -v

# PrÃ¼fen Sie die gespeicherten Dateien
ls -la scraped_data/html/
cat scraped_data/json/meeting_*.json | jq .

# Wenn gut, dann mehr Seiten
python ris_scraper.py <URL> -n 10
```

## ğŸ“ Output-Struktur

```
scraped_data/
â”œâ”€â”€ html/
â”‚   â”œâ”€â”€ meeting_abc123def456.html
â”‚   â”œâ”€â”€ meeting_789xyz123abc.html
â”‚   â””â”€â”€ ...
â”œâ”€â”€ json/
â”‚   â”œâ”€â”€ meeting_abc123def456.json
â”‚   â”œâ”€â”€ meeting_789xyz123abc.json
â”‚   â””â”€â”€ ...
â””â”€â”€ scraping_progress.json
```

**JSON-Format:**
```json
{
  "url": "https://...",
  "title": "Stadtratssitzung vom 15.03.2024",
  "date": "15.03.2024",
  "time": "19:00",
  "location": "Rathaus, GroÃŸer Sitzungssaal",
  "organization": "Stadtrat",
  "agenda_items": [
    {
      "number": "1.1",
      "title": "Bebauungsplan Nr. 45",
      "position": 1
    }
  ],
  "documents": [
    {
      "title": "Beschlussvorlage BP-45",
      "url": "https://.../file.pdf"
    }
  ],
  "html_content": "./scraped_data/html/meeting_abc123.html",
  "scraped_at": "2024-10-28T18:30:00"
}
```

## ğŸ› ï¸ Fortgeschrittene Nutzung

### Fortschritt fortsetzen

Der Scraper speichert seinen Fortschritt automatisch. Bei Unterbrechung:

```bash
# Einfach erneut starten - bereits gescrapte URLs werden Ã¼bersprungen
python ris_scraper.py <URL> -n 100
```

### Rate Limiting anpassen

Manche Systeme blockieren bei zu vielen Requests:

```bash
# Langsamer scrapen (2 Sekunden Pause)
python ris_scraper.py <URL> -r 2.0

# Sehr langsam fÃ¼r sensible Server
python ris_scraper.py <URL> -r 5.0
```

### Mit YAML-Config arbeiten

Erstellen Sie eine `scraper_config.yaml`:

```yaml
base_url: "https://ratsinfo.IhreStadt.de"
output_dir: "./data/meine_stadt"
max_pages: 100
rate_limit_seconds: 1.5
ris_type: "allris"
date_from: "2024-01-01"
date_to: "2024-12-31"
```

Dann laden Sie diese mit Python:

```python
import yaml
from ris_scraper import ScraperConfig, RISScraperFactory

with open('scraper_config.yaml') as f:
    config_dict = yaml.safe_load(f)

config = ScraperConfig(**config_dict)
scraper = RISScraperFactory.create(config)
meetings = scraper.scrape()
```

## ğŸ› Troubleshooting

### Problem: "Connection timeout"

**LÃ¶sung:**
```bash
# ErhÃ¶hen Sie das Timeout
python ris_scraper.py <URL> -r 2.0  # Langsamer scrapen
```

### Problem: "Too many requests (429)"

**LÃ¶sung:**
```bash
# Deutlich lÃ¤ngere Pausen
python ris_scraper.py <URL> -r 5.0
```

### Problem: "Keine Meetings gefunden"

**MÃ¶gliche Ursachen:**
1. Falsche Start-URL
2. RIS-Typ falsch erkannt
3. HTML-Struktur anders als erwartet

**Debug:**
```bash
# 1. Struktur analysieren
python analyze_ris_structure.py <URL>

# 2. Mit verbose-Modus scrapen
python ris_scraper.py <URL> -n 1 -v

# 3. RIS-Typ manuell angeben
python ris_scraper.py <URL> --ris-type allris -v
```

### Problem: "Falsche Daten extrahiert"

Die Selektoren in `ris_scraper.py` mÃ¼ssen angepasst werden:

1. Analysieren Sie die HTML-Struktur: `python analyze_ris_structure.py <URL>`
2. Ã–ffnen Sie eine gespeicherte HTML-Datei im Browser
3. Inspizieren Sie die Elemente (F12 â†’ Developer Tools)
4. Passen Sie die Selektoren in `ris_scraper.py` an
5. Testen Sie mit: `python ris_scraper.py <URL> -n 1 -v`

## ğŸ”— Integration mit ETL-Pipeline

Nach dem Scraping kÃ¶nnen die HTML-Dumps in die PostgreSQL-Datenbank geladen werden:

```bash
# 1. Scrapen
python ris_scraper.py <URL> -n 100 -o ./scraped_data

# 2. ETL-Pipeline ausfÃ¼hren
python stadtrat_etl_pipeline.py

# (stadtrat_etl_pipeline.py muss konfiguriert werden fÃ¼r ./scraped_data)
```

Oder direkt im ETL-Code:

```python
from stadtrat_etl_pipeline import HTMLExtractor, DatabaseLoader

# HTML-Dumps laden
extractor = HTMLExtractor("./scraped_data/html")
meetings = extractor.extract_all_meetings()

# In Datenbank laden
loader = DatabaseLoader(db_config)
for meeting, agenda_items in meetings:
    loader.insert_meeting(meeting)
    loader.insert_agenda_items(agenda_items)
```

## ğŸ“Š Beispiel-Workflow

```bash
# 1. Neue Stadt: Struktur analysieren
python analyze_ris_structure.py https://ratsinfo.neustadt.de > analyse.txt
cat analyse.txt  # Lesen und verstehen

# 2. Test-Scrape mit wenigen Seiten
python ris_scraper.py https://ratsinfo.neustadt.de -n 5 -v

# 3. PrÃ¼fen der Ergebnisse
ls -la scraped_data/html/
head scraped_data/json/meeting_*.json

# 4. Wenn gut: VollstÃ¤ndiges Scraping
python ris_scraper.py https://ratsinfo.neustadt.de -n 200 \
    --date-from 2024-01-01 \
    -o ./data/neustadt

# 5. In ETL-Pipeline laden
python stadtrat_etl_pipeline.py

# 6. Datenbank abfragen
psql -d stadtrat_db -c "SELECT COUNT(*) FROM meetings;"
```

## ğŸš¨ Rechtliche Hinweise

**WICHTIG:**
- PrÃ¼fen Sie die robots.txt der Website: `https://ratsinfo.stadt.de/robots.txt`
- Respektieren Sie Rate Limits (min. 1 Sekunde zwischen Requests)
- Nutzen Sie die Daten nur fÃ¼r private/wissenschaftliche Zwecke
- Kommerzielle Nutzung: Fragen Sie die Stadt um Erlaubnis
- Viele RIS bieten auch OParl-APIs - nutzen Sie diese bevorzugt!

**Best Practices:**
- âœ… Rate Limiting aktivieren (min. 1s)
- âœ… Identifizierbaren User-Agent verwenden
- âœ… Nur Ã¶ffentliche Daten scrapen
- âœ… Server nicht Ã¼berlasten (max. 1 Request/Sekunde)
- âœ… Zwischenspeichern (kein wiederholtes Scraping derselben Daten)

## ğŸ“š WeiterfÃ¼hrende Ressourcen

- **OParl Standard**: https://oparl.org (bevorzugt nutzen!)
- **ALLRIS Dokumentation**: https://www.cc-egov.de
- **SessionNet**: https://sternberg.com
- **BeautifulSoup Docs**: https://www.crummy.com/software/BeautifulSoup/
- **Scrapy (Alternative)**: https://scrapy.org

## ğŸ¤ Support & Contribution

Bei Fragen oder Problemen:
1. PrÃ¼fen Sie die Troubleshooting-Sektion
2. Analysieren Sie die HTML-Struktur mit `analyze_ris_structure.py`
3. Aktivieren Sie verbose-Modus: `-v`

FÃ¼r neue RIS-Systeme: Erstellen Sie eine neue Scraper-Klasse in `ris_scraper.py`

## ğŸ“„ Lizenz

MIT License - Nutzen Sie es frei fÃ¼r Ihre Projekte!

---

**Happy Scraping! ğŸ•·ï¸**
