{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3269d360",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import folium\n",
    "from folium import plugins\n",
    "from IPython.display import display, HTML\n",
    "import rdflib\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from collections import Counter\n",
    "\n",
    "# Setup paths\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "PARQUET_FILE = DATA_DIR / \"council_data.parquet\"\n",
    "GEOJSON_FILE = DATA_DIR / \"augsburg_map.geojson\"\n",
    "NT_FILE = DATA_DIR / \"metadata.nt\"\n",
    "TTL_FILE = DATA_DIR / \"metadata.ttl\"\n",
    "\n",
    "print(\"‚úì Imports loaded\")\n",
    "print(f\"\\nData directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b457f",
   "metadata": {},
   "source": [
    "## 2. Load Parquet Data\n",
    "\n",
    "Load the main council data from Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet\n",
    "if PARQUET_FILE.exists():\n",
    "    df = pd.read_parquet(PARQUET_FILE)\n",
    "    print(f\"‚úì Loaded {len(df)} papers from Parquet\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {PARQUET_FILE}\")\n",
    "    print(\"\\nRun the pipeline first:\")\n",
    "    print(\"  python scripts/run_pipeline.py --test --limit 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae299eaa",
   "metadata": {},
   "source": [
    "## 3. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52857649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview\n",
    "print(\"=\" * 70)\n",
    "print(\"PIPELINE OUTPUT STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTotal papers: {len(df)}\")\n",
    "print(f\"City: {df['city'].unique()[0] if len(df) > 0 else 'N/A'}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Count papers with locations\n",
    "papers_with_locations = df[df['locations'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)]\n",
    "print(f\"\\nPapers with locations: {len(papers_with_locations)} / {len(df)}\")\n",
    "\n",
    "# Total locations\n",
    "total_locations = df['locations'].apply(lambda x: len(x) if isinstance(x, list) else 0).sum()\n",
    "print(f\"Total location mentions: {total_locations}\")\n",
    "\n",
    "# Locations with coordinates\n",
    "geocoded_count = 0\n",
    "for locs in df['locations']:\n",
    "    if isinstance(locs, list):\n",
    "        geocoded_count += sum(1 for loc in locs if loc.get('latitude') and loc.get('longitude'))\n",
    "\n",
    "print(f\"Geocoded locations: {geocoded_count} / {total_locations}\")\n",
    "print(f\"Geocoding success rate: {geocoded_count/total_locations*100:.1f}%\" if total_locations > 0 else \"N/A\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f52564",
   "metadata": {},
   "source": [
    "## 4. Sample Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few papers\n",
    "display_cols = ['id', 'name', 'date', 'city']\n",
    "if all(col in df.columns for col in display_cols):\n",
    "    print(\"\\nFirst 5 papers:\")\n",
    "    display(df[display_cols].head())\n",
    "else:\n",
    "    print(\"\\nAvailable columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85837217",
   "metadata": {},
   "source": [
    "## 5. Location Analysis\n",
    "\n",
    "Analyze the extracted locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394498a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all locations into flat list\n",
    "all_locations = []\n",
    "for idx, row in df.iterrows():\n",
    "    if isinstance(row['locations'], list):\n",
    "        for loc in row['locations']:\n",
    "            all_locations.append({\n",
    "                'name': loc.get('name', ''),\n",
    "                'latitude': loc.get('latitude'),\n",
    "                'longitude': loc.get('longitude'),\n",
    "                'source': loc.get('source', ''),\n",
    "                'paper_id': row['id'],\n",
    "                'paper_title': row.get('name', ''),\n",
    "                'paper_date': row.get('date', '')\n",
    "            })\n",
    "\n",
    "df_locations = pd.DataFrame(all_locations)\n",
    "print(f\"Total location mentions: {len(df_locations)}\")\n",
    "\n",
    "if len(df_locations) > 0:\n",
    "    # Top locations by count\n",
    "    location_counts = df_locations['name'].value_counts().head(10)\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TOP 10 MOST MENTIONED LOCATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    for loc, count in location_counts.items():\n",
    "        print(f\"  {loc:40} {count:3} mentions\")\n",
    "\n",
    "    # Source breakdown\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LOCATIONS BY SOURCE\")\n",
    "    print(\"=\" * 70)\n",
    "    source_counts = df_locations['source'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source:20} {count:3} locations\")\n",
    "\n",
    "    # Geocoding stats\n",
    "    geocoded_locs = df_locations[df_locations['latitude'].notna()]\n",
    "    print(f\"\\nGeocoded: {len(geocoded_locs)} / {len(df_locations)} ({len(geocoded_locs)/len(df_locations)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No locations found in the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489f0b4",
   "metadata": {},
   "source": [
    "## 6. Interactive Map with PDF Links\n",
    "\n",
    "Create an interactive map using Folium. Each location marker includes:\n",
    "- Location name\n",
    "- Paper title and date\n",
    "- **Direct link to original PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GeoJSON\n",
    "if GEOJSON_FILE.exists():\n",
    "    with open(GEOJSON_FILE, 'r', encoding='utf-8') as f:\n",
    "        geojson_data = json.load(f)\n",
    "\n",
    "    print(f\"‚úì Loaded GeoJSON with {len(geojson_data['features'])} features\")\n",
    "\n",
    "    # Create map centered on Augsburg\n",
    "    augsburg_center = [48.3705, 10.8978]\n",
    "    m = folium.Map(\n",
    "        location=augsburg_center,\n",
    "        zoom_start=13,\n",
    "        tiles='OpenStreetMap'\n",
    "    )\n",
    "\n",
    "    # Add markers for each location\n",
    "    for feature in geojson_data['features']:\n",
    "        coords = feature['geometry']['coordinates']\n",
    "        props = feature['properties']\n",
    "\n",
    "        # Build popup HTML with PDF link\n",
    "        popup_html = f\"\"\"\n",
    "        <div style=\"width: 300px;\">\n",
    "            <h4 style=\"margin-bottom: 10px;\">{props.get('location_name', 'Unknown')}</h4>\n",
    "            <hr style=\"margin: 5px 0;\">\n",
    "            <p style=\"margin: 5px 0;\"><strong>Paper:</strong> {props.get('paper_title', 'N/A')}</p>\n",
    "            <p style=\"margin: 5px 0;\"><strong>Date:</strong> {props.get('paper_date', 'N/A')}</p>\n",
    "            <p style=\"margin: 5px 0;\"><strong>Source:</strong> {props.get('source', 'N/A')}</p>\n",
    "        \"\"\"\n",
    "\n",
    "        # Add PDF link if available\n",
    "        if props.get('pdf_url'):\n",
    "            popup_html += f'<p style=\"margin: 10px 0;\"><a href=\"{props[\"pdf_url\"]}\" target=\"_blank\" style=\"color: #0066cc; font-weight: bold;\">üìÑ Open PDF</a></p>'\n",
    "\n",
    "        popup_html += \"</div>\"\n",
    "\n",
    "        # Add marker\n",
    "        folium.Marker(\n",
    "            location=[coords[1], coords[0]],  # GeoJSON is [lon, lat], Folium needs [lat, lon]\n",
    "            popup=folium.Popup(popup_html, max_width=300),\n",
    "            tooltip=props.get('location_name', 'Unknown'),\n",
    "            icon=folium.Icon(color='blue', icon='info-sign')\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Add marker cluster for better performance with many locations\n",
    "    # marker_cluster = plugins.MarkerCluster().add_to(m)\n",
    "\n",
    "    # Display map\n",
    "    display(m)\n",
    "\n",
    "    print(f\"\\n‚úì Map created with {len(geojson_data['features'])} location markers\")\n",
    "    print(\"üí° Click on markers to see paper details and PDF links\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå GeoJSON file not found: {GEOJSON_FILE}\")\n",
    "    print(\"\\nThe pipeline should have created this file automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ec7ba",
   "metadata": {},
   "source": [
    "## 7. RDF Conversion: N-Triples ‚Üí Turtle\n",
    "\n",
    "Convert the RDF output from N-Triples (.nt) to Turtle (.ttl) format for better readability and YASGUI compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load N-Triples and convert to Turtle\n",
    "if NT_FILE.exists():\n",
    "    print(f\"Loading RDF from: {NT_FILE}\")\n",
    "\n",
    "    # Create RDF graph\n",
    "    g = Graph()\n",
    "\n",
    "    # Parse N-Triples\n",
    "    g.parse(str(NT_FILE), format='nt')\n",
    "\n",
    "    print(f\"‚úì Loaded {len(g)} triples\")\n",
    "\n",
    "    # Bind common namespaces for prettier output\n",
    "    g.bind('oparl', Namespace('http://oparl.org/schema/1.1/'))\n",
    "    g.bind('dct', Namespace('http://purl.org/dc/terms/'))\n",
    "    g.bind('geo', Namespace('http://www.opengis.net/ont/geosparql#'))\n",
    "    g.bind('xsd', Namespace('http://www.w3.org/2001/XMLSchema#'))\n",
    "\n",
    "    # Serialize to Turtle\n",
    "    print(f\"\\nConverting to Turtle format...\")\n",
    "    ttl_content = g.serialize(format='turtle')\n",
    "\n",
    "    # Save to file\n",
    "    with open(TTL_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(ttl_content)\n",
    "\n",
    "    print(f\"‚úì Saved Turtle file: {TTL_FILE}\")\n",
    "    print(f\"  Size: {TTL_FILE.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "    # Show sample triples\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SAMPLE TURTLE OUTPUT (first 30 lines):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\n\".join(ttl_content.split('\\n')[:30]))\n",
    "    print(\"\\n... (truncated) ...\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå N-Triples file not found: {NT_FILE}\")\n",
    "    print(\"\\nThe pipeline should have created this file automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae68cbb",
   "metadata": {},
   "source": [
    "## 8. YASGUI Upload Instructions\n",
    "\n",
    "To test the RDF data in YASGUI:\n",
    "\n",
    "1. **Open YASGUI**: https://yasgui.triply.cc/\n",
    "2. **Upload TTL file**: \n",
    "   - Click on \"Data\" tab\n",
    "   - Click \"Upload file\"\n",
    "   - Select `data/processed/metadata.ttl`\n",
    "3. **Run SPARQL queries**\n",
    "\n",
    "### Example SPARQL Queries:\n",
    "\n",
    "```sparql\n",
    "# Query 1: Count all papers\n",
    "PREFIX oparl: <http://oparl.org/schema/1.1/>\n",
    "SELECT (COUNT(?paper) AS ?count)\n",
    "WHERE {\n",
    "  ?paper a oparl:Paper .\n",
    "}\n",
    "\n",
    "# Query 2: List all papers with titles\n",
    "PREFIX oparl: <http://oparl.org/schema/1.1/>\n",
    "PREFIX dct: <http://purl.org/dc/terms/>\n",
    "SELECT ?paper ?title ?date\n",
    "WHERE {\n",
    "  ?paper a oparl:Paper ;\n",
    "         dct:title ?title ;\n",
    "         dct:date ?date .\n",
    "}\n",
    "ORDER BY DESC(?date)\n",
    "\n",
    "# Query 3: Find papers mentioning specific locations\n",
    "PREFIX oparl: <http://oparl.org/schema/1.1/>\n",
    "PREFIX geo: <http://www.opengis.net/ont/geosparql#>\n",
    "SELECT ?paper ?title ?location ?coords\n",
    "WHERE {\n",
    "  ?paper a oparl:Paper ;\n",
    "         dct:title ?title ;\n",
    "         oparl:relatesToArea ?location .\n",
    "  ?location geo:asWKT ?coords .\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80333c63",
   "metadata": {},
   "source": [
    "## 9. File Summary\n",
    "\n",
    "Summary of all generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GENERATED FILES SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "files_to_check = [\n",
    "    (PARQUET_FILE, \"Parquet (main data)\"),\n",
    "    (GEOJSON_FILE, \"GeoJSON (map data)\"),\n",
    "    (NT_FILE, \"N-Triples (RDF)\"),\n",
    "    (TTL_FILE, \"Turtle (RDF, converted)\"),\n",
    "]\n",
    "\n",
    "for file_path, description in files_to_check:\n",
    "    if file_path.exists():\n",
    "        size_kb = file_path.stat().st_size / 1024\n",
    "        print(f\"\\n‚úì {description}\")\n",
    "        print(f\"  Path: {file_path}\")\n",
    "        print(f\"  Size: {size_kb:.1f} KB\")\n",
    "    else:\n",
    "        print(f\"\\n‚úó {description}\")\n",
    "        print(f\"  Path: {file_path}\")\n",
    "        print(f\"  Status: NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51e9e9",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "After validating the test run:\n",
    "\n",
    "1. **Run Full Pipeline** (6 months of data):\n",
    "   ```bash\n",
    "   python scripts/run_pipeline.py --city augsburg\n",
    "   ```\n",
    "\n",
    "2. **Deeper Analysis**:\n",
    "   - Temporal patterns (which areas are mentioned when?)\n",
    "   - Spatial clustering (are decisions focused in certain districts?)\n",
    "   - Topic modeling combined with locations\n",
    "\n",
    "3. **Visualization**:\n",
    "   - Heatmaps of council activity\n",
    "   - Time-series animations\n",
    "   - Network graphs of related locations\n",
    "\n",
    "4. **Export for Research**:\n",
    "   - Share Turtle file with collaborators\n",
    "   - Publish to SPARQL endpoint\n",
    "   - Create interactive web dashboard"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
