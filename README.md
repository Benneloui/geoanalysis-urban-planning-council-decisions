# Geomodelierung - Analysis of Municipal Council Decisions

## Summary

This project investigates municipal council decisions inÂ **Augsburg, Germany**. By leveraging theÂ **OParl API**Â interface, unstructured parliamentary documents (session data and papers) are transformed into structured datasets. The project applies aÂ **hybrid extraction pipeline**Â combining Named Entity Recognition (NER), Fuzzy Matching, and OpenStreetMap validation to geolocate political activities. The analysis aims to reveal patterns of the council and spatial distributions of political attention (e.g., center vs. periphery bias).

## Background and Motivation

Urban planning is a core function of local government, yet the patterns of these decisions often remain hidden in thousands of PDF documents. While digitization standards likeÂ **OParl**Â exist, they are rarely used for quantitative spatial analysis.


## Objectives & Research Questions

**Primary Research Question:**

> "How is political attention distributed spatially across the districts of Augsburg, and what temporal patterns define the council's workflow?"

**Sub-questions:**

1. **Temporal:**Â When does the council meet? Are there significant shifts in meeting frequencies or times over the legislative period (2020â€“2025)?

2. **Spatial:**Â How is political attention (measured by parliamentary activity) distributed spatially across the districts of Augsburg, and does a center-periphery bias exist?


## Methodology & Implementation

The project moves beyond simple keyword searching by implementing aÂ **Python-based ETL pipeline**Â (Extract, Transform, Load).

#### 1. Data

- **Source:**Â Official OParl API of the City of Augsburg (SessionNet).


#### 2. The "Location Extractor"

To solve the problem of unstructured location data in titles (e.g.,Â _"Sanierung der Maxstr."_), a three-stage extraction logic is developed:

1. **NER (Named Entity Recognition):**Â UsingÂ `spaCy`Â (model:Â `de_core_news_sm`) to identify location entities in text context.

2. **Ground Truth Validation:**Â Extracted tokens are matched against a localÂ **OpenStreetMap (OSM) dataset**Â containing all validated street names in Augsburg (via Overpass API).

3. **Fuzzy Matching:**Â Using Levenshtein distance (`thefuzz`) to map typos or abbreviations in documents to the correct OSM street name before geocoding.

## Preliminary Results (Proof of Concept)

A pilot run of the data pipeline has validated the feasibility:

- **Data Base:**Â Successfully harvestedÂ **~750 meetings**Â from Jan 2020 to Nov 2025.

- **Geocoding Success:**Â The streetnames form the meta Date got successfully geocoded to coordinates.


## Challenges
- Augsburg uses NON-STANDARD OParl endpoint names
- need to analyse the Augsburg OParl Standart first

## Tools & Stack

- **Language:**Â Mainly Python (VS Code Environment)

- **Data Fetching:**Â `requests`Â (with Retry-Adapter)

- **NLP & Matching:**Â `spaCy`,Â `thefuzz`

- **Geodata:**Â `geopy`Â (Nominatim),Â `Overpass API`Â (OSM)

- **Analysis/Viz:**Â `pandas`,Â `matplotlib`,Â `folium`

```
Geomodelierung/
â”œâ”€â”€ src/                      # Core modules
â”‚   â”œâ”€â”€ client.py            # OParl API client
â”‚   â”œâ”€â”€ extraction.py        # PDF text extraction
â”‚   â”œâ”€â”€ storage.py           # Parquet/RDF/GeoJSON writers
â”‚   â”œâ”€â”€ state.py             # State management & checkpoints
â”‚   â”œâ”€â”€ spatial.py           # Location extraction & geocoding
â”‚   â”œâ”€â”€ validation.py        # SHACL & data quality validation
â”‚   â””â”€â”€ enrichment.py        # Wikidata/GeoNames/ML enrichment
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.py      # Main orchestration script
â”œâ”€â”€ tests/                    # Test suite
â”‚   â”œâ”€â”€ conftest.py          # Shared fixtures
â”‚   â”œâ”€â”€ test_client.py       # Client tests
â”‚   â”œâ”€â”€ test_extraction.py   # Extraction tests
â”‚   â”œâ”€â”€ test_storage.py      # Storage tests
â”‚   â”œâ”€â”€ test_state.py        # State tests
â”‚   â”œâ”€â”€ test_spatial.py      # Spatial tests
â”‚   â”œâ”€â”€ test_integration.py  # Integration tests
â”‚   â””â”€â”€ run_tests.py         # Test runner
â”œâ”€â”€ data/                     # Output directory
â”‚   â”œâ”€â”€ papers_parquet/      # Parquet datasets
â”‚   â”œâ”€â”€ ttl/                 # RDF/Turtle files
â”‚   â””â”€â”€ *.geojson           # GeoJSON for mapping
â”œâ”€â”€ config.yaml              # Configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ pytest.ini              # Test configuration
â”œâ”€â”€ QUICKSTART.md           # Quick start guide
â”œâ”€â”€ TESTING.md              # Testing documentation
â”œâ”€â”€ ENRICHMENT.md           # Enrichment documentation
â””â”€â”€ IMPLEMENTATION_SUMMARY.md

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Submit a pull request

## ğŸ“ License

- **Code:** MIT License
- **Documentation:** CC-BY 4.0

## ğŸ‘¤ Contact

**Benedikt Pilgram**
- GitHub: [@benneloui](https://github.com/benneloui)


